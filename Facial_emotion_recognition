{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1351797,"sourceType":"datasetVersion","datasetId":786787},{"sourceId":1732825,"sourceType":"datasetVersion","datasetId":1028436}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":8045.909776,"end_time":"2025-09-10T18:08:40.969022","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-09-10T15:54:35.059246","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yomnagharib/last-fer0-94?scriptVersionId=261335179\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Build mixed FER/EDFER splits as requested:\n# TRAIN = all EDFER + 75% FER2013/train\n# VAL   = 25% FER2013/train\n# TEST  = FER2013/test (or PrivateTest)\nfrom pathlib import Path\nimport shutil, random\n\n# --- Config ---\nEDFER_ROOT   = Path(\"/kaggle/input/emotion-detection-fer\")\nFER2013_ROOT = Path(\"/kaggle/input/fer2013\")\nOUT_ROOT     = Path(\"/kaggle/content/fer2013_by_usage_1\")\n\nIMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\nSEED = 42\nFER_TRAIN_FRAC_FOR_TRAIN = 0.75   # remaining 0.25 -> val\n\n# EDFER label mapping\nEDFER_MAP = {\"disgusted\": \"disgust\", \"fearful\": \"fear\", \"surprised\": \"surprise\"}\n\n# --- Helpers ---\ndef is_img(p: Path) -> bool:\n    return p.is_file() and p.suffix.lower() in IMG_EXTS\n\ndef map_edfer(label: str) -> str:\n    l = label.strip().lower()\n    return EDFER_MAP.get(l, l)\n\ndef norm_fer(label: str) -> str:\n    return label.strip().lower()\n\ndef ensure_dirs(*dirs: Path):\n    for d in dirs: d.mkdir(parents=True, exist_ok=True)\n\ndef class_dirs(root: Path):\n    return [d for d in sorted(root.iterdir()) if d.is_dir()]\n\ndef copy_all(files, dst_dir: Path, prefix: str):\n    ensure_dirs(dst_dir)\n    for f in files:\n        shutil.copy2(f, dst_dir / f\"{prefix}__{f.name}\")\n\ndef find_first_dir(root: Path, names):\n    for n in names:\n        p = root / n\n        if p.is_dir(): return p\n    return None\n\n# --- Output skeleton ---\nOUT_TRAIN, OUT_VAL, OUT_TEST = OUT_ROOT/\"train\", OUT_ROOT/\"val\", OUT_ROOT/\"test\"\nensure_dirs(OUT_TRAIN, OUT_VAL, OUT_TEST)\n\nif not EDFER_ROOT.exists():\n    raise FileNotFoundError(f\"Missing EDFER at {EDFER_ROOT}\")\nif not FER2013_ROOT.exists():\n    raise FileNotFoundError(f\"Missing FER2013 at {FER2013_ROOT}\")\n\nrng = random.Random(SEED)\n\n# === 1) EDFER: put the ENTIRE dataset into TRAIN ===\n# We’ll copy from any present split folders into OUT_TRAIN (with label mapping).\ndef copy_entire_edfer_to_train(edfer_root: Path):\n    split_candidates = [\"train\",\"Train\",\"val\",\"validation\",\"valid\",\"Val\",\"Validation\",\"Valid\",\"test\",\"Test\",\"PrivateTest\"]\n    # Use any subdir that contains class folders\n    for name in split_candidates:\n        sroot = edfer_root / name\n        if not sroot.is_dir(): \n            continue\n        for cls in class_dirs(sroot):\n            label = map_edfer(cls.name)\n            files = [p for p in sorted(cls.iterdir()) if is_img(p)]\n            if not files: \n                continue\n            copy_all(files, OUT_TRAIN/label, prefix=\"edfer\")\n\ncopy_entire_edfer_to_train(EDFER_ROOT)\n\n# === 2) FER2013: split TRAIN 75/25 per-class into TRAIN/VAL ===\nfer_train = find_first_dir(FER2013_ROOT, [\"train\",\"Train\",\"Training\"])\nif fer_train is None:\n    raise FileNotFoundError(f\"No FER2013 train split found under {FER2013_ROOT}\")\n\nfor cls in class_dirs(fer_train):\n    label = norm_fer(cls.name)\n    files = [p for p in sorted(cls.iterdir()) if is_img(p)]\n    if not files:\n        continue\n    # deterministic per-class shuffle\n    r = random.Random(SEED + hash(label) % (2**31))\n    r.shuffle(files)\n    k = max(1, int(round(len(files) * FER_TRAIN_FRAC_FOR_TRAIN)))\n    train_files, val_files = files[:k], files[k:]\n    copy_all(train_files, OUT_TRAIN/label, prefix=\"fer2013_tr\")\n    copy_all(val_files,   OUT_VAL/label,   prefix=\"fer2013_val\")\n\n# === 3) FER2013: TEST split ===\nfer_test = find_first_dir(FER2013_ROOT, [\"test\",\"Test\",\"PrivateTest\"])\nif fer_test is None:\n    raise FileNotFoundError(f\"No FER2013 test/PrivateTest split found under {FER2013_ROOT}\")\n\nfor cls in class_dirs(fer_test):\n    label = norm_fer(cls.name)\n    files = [p for p in sorted(cls.iterdir()) if is_img(p)]\n    if files:\n        copy_all(files, OUT_TEST/label, prefix=\"fer2013_test\")\n\n# --- Reporting (compact) ---\ndef count_split(root: Path):\n    per_class = {}\n    examples = []\n    if root.exists():\n        for cls in class_dirs(root):\n            imgs = [p for p in sorted(cls.iterdir()) if is_img(p)]\n            per_class[cls.name] = len(imgs)\n            examples.extend(imgs[:3])\n    return per_class, [str(p) for p in examples[:6]]\n\ntr_c, tr_ex = count_split(OUT_TRAIN)\nva_c, va_ex = count_split(OUT_VAL)\nte_c, te_ex = count_split(OUT_TEST)\n\nprint(\"=== Summary ===\")\nprint(\"Output:\", OUT_ROOT)\nprint(\"Totals -> train:\", sum(tr_c.values()), \" val:\", sum(va_c.values()), \" test:\", sum(te_c.values()))\nprint(\"\\nPer-class (train/val/test):\")\nfor lbl in sorted(set(tr_c)|set(va_c)|set(te_c)):\n    print(f\"{lbl:>10s}  {tr_c.get(lbl,0):5d} / {va_c.get(lbl,0):5d} / {te_c.get(lbl,0):5d}\")\nprint(\"\\nSamples:\")\nprint(\" train:\", tr_ex[:5])\nprint(\" val  :\", va_ex[:5])\nprint(\" test :\", te_ex[:5])\n","metadata":{"execution":{"iopub.status.busy":"2025-09-11T23:50:01.870992Z","iopub.execute_input":"2025-09-11T23:50:01.871392Z","iopub.status.idle":"2025-09-12T00:05:07.28705Z","shell.execute_reply.started":"2025-09-11T23:50:01.871365Z","shell.execute_reply":"2025-09-12T00:05:07.286415Z"},"papermill":{"duration":397.016392,"end_time":"2025-09-10T16:01:16.021066","exception":false,"start_time":"2025-09-10T15:54:39.004674","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nOUT_ROOT = \"/kaggle/content/fer2013_by_usage_1\"\nIMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\")\nLABEL_ORDER = ['angry','disgust','fear','happy','sad','surprise','neutral']\nNUM_TO_NAME = {0:'anger',1:'disgust',2:'fear',3:'happy',4:'sad',5:'surprise',6:'neutral'}\n\ndef count_classes(split_dir):\n    counts = {}\n    if not os.path.isdir(split_dir):\n        return counts\n    for cls in sorted(os.listdir(split_dir)):\n        p = os.path.join(split_dir, cls)\n        if not os.path.isdir(p): \n            continue\n        # count image files\n        num = sum(1 for f in os.listdir(p) if f.lower().endswith(IMG_EXTS))\n        counts[cls] = num\n    return counts\n\ntrain_dir = os.path.join(OUT_ROOT, \"train\")\nval_dir   = os.path.join(OUT_ROOT, \"val\")\ntest_dir  = os.path.join(OUT_ROOT, \"test\")\n\ntrain_counts = count_classes(train_dir)\nval_counts   = count_classes(val_dir)\ntest_counts  = count_classes(test_dir)\n\ndef pretty_print(name, counts):\n    total = sum(counts.values())\n    print(f\"\\n--- {name.upper()} (total {total}) ---\")\n    if not counts:\n        return\n    # Print in canonical LABEL_ORDER when possible, otherwise fallback to sorted keys\n    keys = LABEL_ORDER if any(k in counts for k in LABEL_ORDER) else sorted(counts.keys())\n    for k in keys:\n        if k in counts:\n            # show folder name and friendly label if numeric\n            base = k.split('_')[0]\n            try:\n                idx = int(base)\n                label = NUM_TO_NAME.get(idx, base)\n            except:\n                label = k\n            print(f\"  {k:12} -> {label:8} : {counts[k]}\")\n    # print any other classes not in LABEL_ORDER\n    for k in sorted(counts.keys()):\n        if k not in keys:\n            print(f\"  {k:12} -> {k:8} : {counts[k]}\")\n\npretty_print(\"train\", train_counts)\npretty_print(\"val\", val_counts)\npretty_print(\"test\", test_counts)\n\n# Plot per-class bars side-by-side using LABEL_ORDER\nall_classes = LABEL_ORDER if any(k in train_counts or k in val_counts or k in test_counts for k in LABEL_ORDER) else sorted(set(list(train_counts.keys())+list(val_counts.keys())+list(test_counts.keys())))\nfig, axs = plt.subplots(1, 3, figsize=(16,4), constrained_layout=True)\nfor ax, (name, counts) in zip(axs, [(\"train\", train_counts), (\"val\", val_counts), (\"test\", test_counts)]):\n     vals = [counts.get(c, 0) for c in all_classes]\n     ax.bar(range(len(all_classes)), vals)\n     ax.set_title(f\"{name} (total {sum(vals)})\")\n     ax.set_xticks(range(len(all_classes)))\n     ax.set_xticklabels(all_classes, rotation=45, ha='right')\n     ax.set_ylabel(\"num images\")\nplt.suptitle(\"Per-class image counts by split (FER-2013 Usage)\")\nplt.show()\n\n# Show up to 3 example images (one per split) to sanity-check paths\ndef find_first_image(split_dir):\n    if not os.path.isdir(split_dir):\n        return None\n    for cls in sorted(os.listdir(split_dir)):\n        p = os.path.join(split_dir, cls)\n        if not os.path.isdir(p): continue\n        for fn in sorted(os.listdir(p)):\n            if fn.lower().endswith(IMG_EXTS):\n                return os.path.join(p, fn)\n    return None\n\nsample_paths = [\n    (\"train\", find_first_image(train_dir)),\n    (\"val\",   find_first_image(val_dir)),\n    (\"test\",  find_first_image(test_dir))\n]\n\n# display images \nfig = None\nimgs_to_show = [p for _, p in sample_paths if p]\nif imgs_to_show:\n    n = len(imgs_to_show)\n    fig, axs = plt.subplots(1, n, figsize=(n*3,3))\n    if n == 1: axs = [axs]\n    for ax, img_path in zip(axs, imgs_to_show):\n        try:\n            im = Image.open(img_path)\n            ax.imshow(im)\n            ax.axis('off')\n            ax.set_title(os.path.basename(os.path.dirname(img_path)))\n        except Exception as e:\n            ax.text(0.5,0.5,str(e))\n    plt.show()\nelse:\n    print(\"No sample images found to display.\")\n","metadata":{"execution":{"iopub.status.busy":"2025-09-12T00:05:07.288057Z","iopub.execute_input":"2025-09-12T00:05:07.288329Z","iopub.status.idle":"2025-09-12T00:05:08.054547Z","shell.execute_reply.started":"2025-09-12T00:05:07.288311Z","shell.execute_reply":"2025-09-12T00:05:08.053935Z"},"papermill":{"duration":0.780524,"end_time":"2025-09-10T16:01:16.804748","exception":false,"start_time":"2025-09-10T16:01:16.024224","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, math, random, warnings\nfrom PIL import Image\nimport numpy as np\n\n# config (same as before)\nOUT_ROOT = globals().get(\"OUT_ROOT\", \"/content/fer2013_by_usage_1\")\nTRAIN_DIR = os.path.join(OUT_ROOT, \"train\")\nIMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\")\nSEED = int(globals().get(\"SEED\", 42))\nTARGET_PER_CLASS = 7000\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\ndef is_image_file(fn):\n    return fn.lower().endswith(IMG_EXTS)\n\n# sanity checks\nif not os.path.isdir(TRAIN_DIR):\n    raise FileNotFoundError(f\"Train directory not found at {TRAIN_DIR}. Run the dataset prep cell first.\")\n\n# gather class folders\nclasses = [d for d in sorted(os.listdir(TRAIN_DIR)) if os.path.isdir(os.path.join(TRAIN_DIR, d))]\nif not classes:\n    raise RuntimeError(f\"No class folders found under {TRAIN_DIR}.\")\n\nprint(f\"Found {len(classes)} classes under:\\n  {TRAIN_DIR}\\nGlobal target per class = {TARGET_PER_CLASS}\\n(Disgust class target will be 1200)\\n\")\n\nclasses_to_augment = []\n\n# iterate and show diagnostics\nfor cls in classes:\n    cls_path = os.path.join(TRAIN_DIR, cls)\n    all_files = [f for f in sorted(os.listdir(cls_path)) if is_image_file(f)]\n    orig_files = [f for f in all_files if not f.startswith(\"aug_\")]\n    gen_files  = [f for f in all_files if f.startswith(\"aug_\")]\n\n    num_orig = len(orig_files)\n    num_gen  = len(gen_files)\n    total    = num_orig + num_gen\n\n    # decide per-class target: disgust -> 1200, others -> TARGET_PER_CLASS\n    cls_low = str(cls).lower()\n    is_disgust = (cls_low == '1' or 'disgust' in cls_low)\n    target_for_class = 1200 if is_disgust else TARGET_PER_CLASS\n\n    # compute needed based on total (orig + generated). This prevents re-augmenting if existing aug_ files already reach target.\n    needed = max(0, int(target_for_class - total))\n\n    print(f\"Class: '{cls}'\")\n    print(f\"  originals   : {num_orig}\")\n    print(f\"  generated   : {num_gen}\")\n    print(f\"  total files : {total}\")\n    print(f\"  target      : {target_for_class}\")\n    print(f\"  needed      : {needed}\")\n\n\n    # Mark augmentation only if needed > 0\n    if needed > 0:\n        classes_to_augment.append({\n            \"class\": cls,\n            \"path\": cls_path,\n            \"orig\": num_orig,\n            \"generated\": num_gen,\n            \"total\": total,\n            \"needed\": needed,\n            \"num_src\": num_orig,\n            \"orig_files_sample\": orig_files[:5]\n        })\n        print(f\"  => MARKED for augmentation: need {needed} more images .\")\n    else:\n        print(f\"  => No augmentation needed (total >= {target_for_class}).\")\n\n    print(\"\")  # blank line for readability\n\n# Save to globals for augmentation cell\nglobals()[\"CLASSES_TO_AUGMENT\"] = classes_to_augment\nglobals()[\"TARGET_PER_CLASS\"] = TARGET_PER_CLASS\nglobals()[\"SEED\"] = SEED\n\n# summary\nprint(\"SUMMARY:\")\nprint(f\"  classes found         = {len(classes)}\")\nprint(f\"  classes to augment    = {len(classes_to_augment)}\")\nif classes_to_augment:\n    print(\"  To be augmented:\")\n    for info in classes_to_augment:\n        print(f\"   - {info['class']}: originals={info['orig']}, generated={info['generated']}, total={info['total']}, needed={info['needed']}\")\nelse:\n    print(\"\\nNo classes marked for augmentation.\")\n   \n","metadata":{"execution":{"iopub.status.busy":"2025-09-12T00:05:08.055253Z","iopub.execute_input":"2025-09-12T00:05:08.055459Z","iopub.status.idle":"2025-09-12T00:05:08.12508Z","shell.execute_reply.started":"2025-09-12T00:05:08.055438Z","shell.execute_reply":"2025-09-12T00:05:08.124545Z"},"papermill":{"duration":0.073593,"end_time":"2025-09-10T16:01:16.883183","exception":false,"start_time":"2025-09-10T16:01:16.80959","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Augment cell: set disgust target=1200, compute needed from originals only,\nimport os, math, random, warnings\nfrom PIL import Image\nimport numpy as np\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=r\"albumentations\\.check_version\")\nIMAGE_SIZE=48\n\n# install if missing\ntry:\n    import albumentations as A\n    import cv2\nexcept Exception:\n    import sys\n    print(\"Installing albumentations and opencv-python-headless...\")\n    !{sys.executable} -m pip install -q albumentations opencv-python-headless\n    import albumentations as A\n\n# load config from globals (set by prior detection cell)\nclasses_to_augment = globals().get(\"CLASSES_TO_AUGMENT\", [])\nGLOBAL_TARGET = int(globals().get(\"TARGET_PER_CLASS\", 7000))\nSEED = int(globals().get(\"SEED\", 42))\n\nif not classes_to_augment:\n    print(\"No classes to augment (CLASSES_TO_AUGMENT is empty).\")\nelse:\n    transform = A.Compose([\n    # high-quality resize first (use INTER_CUBIC for nicer result)\n    A.Resize(IMAGE_SIZE, IMAGE_SIZE, interpolation=cv2.INTER_CUBIC),\n\n    # small geometry\n    A.HorizontalFlip(p=0.35),\n    A.Affine(\n        translate_percent={\"x\":(-0.03,0.03), \"y\":(-0.03,0.03)},\n        scale=(0.97, 1.03),\n        rotate=(-6, 6),\n        shear=(-2,2),\n        fit_output=False,\n        p=0.55\n    ),\n\n    # local contrast / global contrast + brightness (use RandomBrightnessContrast instead of missing ops)\n    A.OneOf([\n        A.CLAHE(clip_limit=1.5, tile_grid_size=(8,8), p=0.6),\n        A.RandomBrightnessContrast(brightness_limit=0.06, contrast_limit=0.08, p=0.6),\n    ], p=0.85),\n\n    # tone/gamma tweak\n    A.RandomGamma(gamma_limit=(90,110), p=0.35),\n\n    # subtle color tweaks\n    A.OneOf([\n        A.HueSaturationValue(hue_shift_limit=2, sat_shift_limit=4, val_shift_limit=3, p=0.25),\n        A.RGBShift(r_shift_limit=2, g_shift_limit=2, b_shift_limit=2, p=0.25)\n    ], p=0.25),\n\n    # sharpening (UnsharpMask + Sharpen alternative)\n    A.OneOf([\n        A.UnsharpMask(blur_limit=3, strength=0.8, p=0.45),\n        A.Sharpen(alpha=(0.08, 0.22), lightness=(0.8, 1.05), p=0.45),\n    ], p=0.6),\n\n    # mild global brightness/contrast (use the supported combined op)\n    A.RandomBrightnessContrast(brightness_limit=0.03, contrast_limit=0.05, p=0.35),\n], p=1.0)\n\n    def augment_image_pil(pil_img, seed_for_image):\n        im = pil_img.convert('RGB')\n        arr = np.array(im)\n        np.random.seed(int(seed_for_image) & 0x7fffffff)\n        augmented = transform(image=arr)['image']\n        return Image.fromarray(augmented)\n\n    # Process each class entry — compute needed from original files only (ignore aug_)\n    for info in classes_to_augment:\n        cls = info[\"class\"]\n        cls_path = info[\"path\"]\n\n        # detect if this is the 'disgust' class: either folder named '1' or name contains 'disgust'\n        cls_low = str(cls).lower()\n        is_disgust = (cls_low == '1' or 'disgust' in cls_low)\n\n        target_for_this_class = 1200 if is_disgust else GLOBAL_TARGET\n\n        # list files and separate original vs generated\n        all_files = [f for f in sorted(os.listdir(cls_path)) if f.lower().endswith((\".jpg\",\".jpeg\",\".png\",\".bmp\"))]\n        orig_files = [f for f in all_files if not f.startswith(\"aug_\")]\n        gen_files  = [f for f in all_files if f.startswith(\"aug_\")]\n\n        num_orig = len(orig_files)\n        needed = max(0, int(target_for_this_class - num_orig))   # compute from originals only\n\n        print(f\"\\nClass '{cls}': orig={num_orig}, generated={len(gen_files)}, target={target_for_this_class}, will generate={needed}\")\n\n        if needed <= 0:\n            print(\"  Skipping — no generation needed for this class (originals >= target).\")\n            continue\n\n        if num_orig == 0:\n            print(\"  Warning: no original source images available for this class — skipping.\")\n            continue\n\n        per_src = math.ceil(needed / num_orig)\n        gen = 0\n        class_hash = abs(hash(cls)) % (2**31)\n        progress_check = max(1, needed // 10)\n\n        # generate images deterministically but skip if file already exists\n        while gen < needed:\n            for src_fn in orig_files:\n                if gen >= needed:\n                    break\n                src_fp = os.path.join(cls_path, src_fn)\n                try:\n                    with Image.open(src_fp) as im:\n                        for j in range(per_src):\n                            if gen >= needed:\n                                break\n                            seed_for_image = SEED + class_hash + gen\n                            base = os.path.splitext(src_fn)[0]\n                            out_name = f\"aug_{SEED}_{class_hash}_{gen}_{base}.png\"\n                            out_fp = os.path.join(cls_path, out_name)\n                            if os.path.exists(out_fp):\n                                gen += 1\n                                continue\n                            aug_im = augment_image_pil(im, seed_for_image)\n                            try:\n                                aug_im.save(out_fp, format='PNG', compress_level=6)\n                                gen += 1\n                                if gen % progress_check == 0 or gen == needed:\n                                    print(f\"   generated {gen}/{needed}\", end='\\r', flush=True)\n                            except Exception as e:\n                                print(f\"   Warning: failed to save {out_fp}: {e}\")\n                except Exception as e:\n                    print(f\"  Warning: skipping source {src_fp}: {e}\")\n\n        final = sum(1 for f in os.listdir(cls_path) if f.lower().endswith((\".jpg\",\".jpeg\",\".png\",\".bmp\")))\n        print(f\"  done -> final files in folder = {final} (target was {target_for_this_class})\")\n\n","metadata":{"execution":{"iopub.status.busy":"2025-09-12T00:05:08.126459Z","iopub.execute_input":"2025-09-12T00:05:08.126783Z","iopub.status.idle":"2025-09-12T00:05:16.988324Z","shell.execute_reply.started":"2025-09-12T00:05:08.126768Z","shell.execute_reply":"2025-09-12T00:05:16.987702Z"},"papermill":{"duration":8.898452,"end_time":"2025-09-10T16:01:25.787053","exception":false,"start_time":"2025-09-10T16:01:16.888601","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  plot before (approx) vs after counts and show some augmented samples\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nOUT_ROOT = \"/kaggle/content/fer2013_by_usage_1\"\nTRAIN_DIR = os.path.join(OUT_ROOT, \"train\")\nIMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\")\n\ndef is_image(fn):\n    return fn.lower().endswith(IMG_EXTS)\n\nclasses = [d for d in sorted(os.listdir(TRAIN_DIR)) if os.path.isdir(os.path.join(TRAIN_DIR, d))]\ncounts_after = {c: sum(1 for f in os.listdir(os.path.join(TRAIN_DIR,c)) if is_image(f)) for c in classes}\n# approximate before by excluding aug_ files (if we copied train from original, these won't exist)\ncounts_before = {}\nfor c in classes:\n    p = os.path.join(TRAIN_DIR, c)\n    files = [f for f in os.listdir(p) if is_image(f)]\n    counts_before[c] = len([f for f in files if not f.startswith(\"aug_\")])\n\nLABEL_ORDER = ['angry','disgust','fear','happy','sad','surprise','neutral']\nall_classes = LABEL_ORDER if any(k in counts_after for k in LABEL_ORDER) else sorted(classes)\nfor c in sorted(classes):\n    if c not in all_classes:\n        all_classes.append(c)\n\nbefore_vals = [counts_before.get(c, 0) for c in all_classes]\nafter_vals  = [counts_after.get(c, 0) for c in all_classes]\n\nx = range(len(all_classes))\nfig, axs = plt.subplots(1, 2, figsize=(16,5), constrained_layout=True)\naxs[0].bar(x, before_vals)\naxs[0].set_xticks(x); axs[0].set_xticklabels(all_classes, rotation=45, ha='right')\naxs[0].set_title(\"Train counts BEFORE\"); axs[0].set_ylabel(\"num images\")\naxs[1].bar(x, after_vals)\naxs[1].set_xticks(x); axs[1].set_xticklabels(all_classes, rotation=45, ha='right')\naxs[1].set_title(\"Train counts AFTER augmentation\")\nplt.suptitle(\"Per-class counts: before vs after (train)\")\nplt.show()\n\nsample_classes = [c for c in classes if any(f.startswith(\"aug_\") for f in os.listdir(os.path.join(TRAIN_DIR,c)))]\nsample_classes = sample_classes[:3]  # show up to 3 classes\nif not sample_classes:\n    print(\"No 'aug_' images found to display.\")\nelse:\n    for cls in sample_classes:\n        p = os.path.join(TRAIN_DIR, cls)\n        aug_files = [f for f in sorted(os.listdir(p)) if is_image_file(f) and f.startswith(\"aug_\")]\n        aug_files = aug_files[:6]  # up to 6 per class\n        if not aug_files:\n            continue\n        n = len(aug_files)\n        cols = min(3,n); rows = (n + cols - 1)//cols\n        plt.figure(figsize=(cols*3, rows*3))\n        for i,fn in enumerate(aug_files):\n            fp = os.path.join(p, fn)\n            try:\n                im = Image.open(fp)\n                plt.subplot(rows, cols, i+1)\n                plt.imshow(im)\n                plt.axis('off')\n                plt.title(f\"{cls} / {fn}\", fontsize=8)\n            except Exception as e:\n                print(\"  Error opening\", fp, e)\n        plt.suptitle(f\"Augmented samples from class '{cls}'\")\n        plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2025-09-12T00:05:16.989052Z","iopub.execute_input":"2025-09-12T00:05:16.989397Z","iopub.status.idle":"2025-09-12T00:05:18.27087Z","shell.execute_reply.started":"2025-09-12T00:05:16.98938Z","shell.execute_reply":"2025-09-12T00:05:18.270168Z"},"papermill":{"duration":1.31294,"end_time":"2025-09-10T16:01:27.106","exception":false,"start_time":"2025-09-10T16:01:25.79306","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def undersample_train(\n    train_dir,\n    out_dir,\n    target_per_class=9000,\n    seed=42,\n    img_exts=(\".jpg\", \".jpeg\", \".png\", \".bmp\"),\n    copy_files=True\n):\n    \n    import os, shutil, random\n    os.makedirs(out_dir, exist_ok=True)\n    rng_global = random.Random(int(seed))\n\n    def is_image(fn):\n        return fn.lower().endswith(img_exts)\n\n    before_counts = {}\n    after_counts = {}\n\n    classes = [d for d in sorted(os.listdir(train_dir)) if os.path.isdir(os.path.join(train_dir, d))]\n    for cls in classes:\n        src_cls = os.path.join(train_dir, cls)\n        dst_cls = os.path.join(out_dir, cls)\n        os.makedirs(dst_cls, exist_ok=True)\n\n        files = [f for f in sorted(os.listdir(src_cls)) if is_image(f)]\n        before_counts[cls] = len(files)\n\n        if len(files) <= target_per_class:\n            sel = files\n        else:\n            # deterministic class-specific sampling\n            cls_seed = int(seed) + (abs(hash(cls)) % (2**31))\n            rng = random.Random(cls_seed)\n            files_shuf = list(files)\n            rng.shuffle(files_shuf)\n            sel = files_shuf[:target_per_class]\n\n        copied = 0\n        for fn in sel:\n            src_fp = os.path.join(src_cls, fn)\n            dst_fp = os.path.join(dst_cls, fn)\n            if os.path.exists(dst_fp):\n                copied += 1\n                continue\n            try:\n                if copy_files:\n                    shutil.copy2(src_fp, dst_fp)\n                else:\n                    shutil.move(src_fp, dst_fp)\n                copied += 1\n            except Exception as e:\n                print(f\"Warning: failed to copy/move {src_fp} -> {dst_fp}: {e}\")\n\n        after_counts[cls] = copied\n\n    # print brief summary\n    total_before = sum(before_counts.values())\n    total_after = sum(after_counts.values())\n    print(f\"Undersample summary: total before = {total_before}, total after = {total_after}\")\n    for cls in sorted(before_counts.keys()):\n        b = before_counts[cls]; a = after_counts.get(cls, 0)\n        if b != a:\n            print(f\"  {cls:12}: before={b:6}  after={a:6}\")\n    return before_counts, after_counts\n\n# Example usage:\nOUT_UNDER = os.path.join(OUT_ROOT, \"train_undersampled\")\nbefore, after = undersample_train(TRAIN_DIR, OUT_UNDER, target_per_class=9000, seed=SEED, copy_files=True)\n#","metadata":{"execution":{"iopub.status.busy":"2025-09-12T00:41:24.337754Z","iopub.execute_input":"2025-09-12T00:41:24.338009Z","iopub.status.idle":"2025-09-12T00:41:24.684694Z","shell.execute_reply.started":"2025-09-12T00:41:24.337989Z","shell.execute_reply":"2025-09-12T00:41:24.683754Z"},"papermill":{"duration":6.433186,"end_time":"2025-09-10T16:01:33.551043","exception":false,"start_time":"2025-09-10T16:01:27.117857","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 1\nimport os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom pathlib import Path\n\n# ---------------- USER CONFIG ----------------\nSEED = 42\nOUT_ROOT = \"/kaggle/content/fer2013_by_usage_1\"\nTRAIN_DIR = os.path.join(OUT_ROOT, \"train_undersampled\")\nVAL_DIR   = os.path.join(OUT_ROOT, \"val\")\nBATCH_SIZE = 64\nIMAGE_SIZE = 224\nAUTOTUNE = tf.data.AUTOTUNE\n\nEPOCHS = 30\nFIRST_PHASE_EPOCHS = 10 # freeze backbone for these\nSECOND_PHASE_EPOCHS = EPOCHS - FIRST_PHASE_EPOCHS\n\nLOCAL_WEIGHTS_ENV = \"PRETRAINED_WEIGHTS_PATH\"\nCLASS_WEIGHTS_PATH_ENV = \"CLASS_WEIGHTS_PATH\"\n# ------------------------------------------------\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\n# --- datasets ---\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    TRAIN_DIR, labels='inferred', label_mode='int', batch_size=BATCH_SIZE,\n    image_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, seed=SEED\n)\nval_ds = None\nif os.path.isdir(VAL_DIR):\n    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n        VAL_DIR, labels='inferred', label_mode='int', batch_size=BATCH_SIZE,\n        image_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=False, seed=SEED\n    )\n\nclass_names = train_ds.class_names\nnum_classes = len(class_names)\nprint(\"Classes:\", class_names)\nprint(\"Num classes:\", num_classes)\n\n# compute counts per class (used for fallback class-weights)\ndef count_images_per_class(root_dir, class_names):\n    counts = []\n    root = Path(root_dir)\n    for c in class_names:\n        p = root / c\n        if not p.exists():\n            counts.append(0)\n        else:\n            files = [f for f in p.iterdir() if f.is_file()]\n            counts.append(len(files))\n    return np.array(counts, dtype=np.int64)\n\ndisk_counts = count_images_per_class(TRAIN_DIR, class_names)\nprint(\"Train counts per class :\", disk_counts)\n\n# --- augmentation & preprocessing ---\ndata_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\"),            \n    tf.keras.layers.RandomRotation(0.08),                \n    tf.keras.layers.RandomTranslation(0.06, 0.06),       \n    tf.keras.layers.RandomZoom((-0.04, 0.04)),           \n    tf.keras.layers.RandomContrast(0.06),   \n])\npreprocess_input = tf.keras.applications.resnet.preprocess_input\n\ndef prepare_for_training(image, label, augment=True):\n    image = tf.cast(image, tf.float32)\n    if augment:\n        image = data_augmentation(image)\n    image = preprocess_input(image)\n    return image, label\n\ndef prepare_for_eval(image, label):\n    image = tf.cast(image, tf.float32)\n    image = preprocess_input(image)\n    return image, label\n\n# Map/pre-fetch datasets\ntrain_ds = train_ds.map(lambda x, y: prepare_for_training(x, y, augment=True), num_parallel_calls=AUTOTUNE)\ntrain_ds = train_ds.prefetch(AUTOTUNE)\nif val_ds:\n    val_ds = val_ds.map(lambda x, y: prepare_for_eval(x, y), num_parallel_calls=AUTOTUNE)\n    val_ds = val_ds.prefetch(AUTOTUNE)\n","metadata":{"execution":{"iopub.status.busy":"2025-09-12T00:41:36.679034Z","iopub.execute_input":"2025-09-12T00:41:36.679334Z","iopub.status.idle":"2025-09-12T00:41:39.787372Z","shell.execute_reply.started":"2025-09-12T00:41:36.679312Z","shell.execute_reply":"2025-09-12T00:41:39.786779Z"},"papermill":{"duration":18.826569,"end_time":"2025-09-10T16:01:52.387726","exception":false,"start_time":"2025-09-10T16:01:33.561157","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 2\nimport numpy as np\nimport tensorflow as tf\n\n# --- helper: load class weights from file or fallback to inverse-frequency ---\ndef load_class_weights_from_file(path, expected_len):\n    path = os.path.expanduser(path)\n    ext = os.path.splitext(path)[1].lower()\n    if ext in (\".csv\", \".txt\"):\n        try:\n            arr = np.loadtxt(path, delimiter=',')\n        except Exception:\n            arr = np.loadtxt(path)\n    elif ext == \".json\":\n        with open(path, 'r') as f:\n            data = json.load(f)\n        if isinstance(data, dict):\n            arr = np.array(list(data.values()), dtype=np.float32)\n        else:\n            arr = np.array(data, dtype=np.float32)\n    else:\n        arr = np.loadtxt(path)\n    arr = np.array(arr, dtype=np.float32).reshape(-1)\n    if arr.shape[0] != expected_len:\n        raise ValueError(f\"Loaded class-weights length {arr.shape[0]} != expected {expected_len}\")\n    return arr\n\ndef get_class_weight_dict(num_classes, try_path=None):\n    if try_path:\n        try:\n            arr = load_class_weights_from_file(try_path, num_classes)\n            print(\"Loaded class weights from file:\", try_path)\n            arr = arr / np.mean(arr)\n            return {i: float(arr[i]) for i in range(num_classes)}\n        except Exception as e:\n            print(\"Failed to load class weights from file:\", e)\n    eps = 1e-6\n    inv = 1.0 / (disk_counts + eps)\n    inv = inv / np.mean(inv)\n    print(\"Using computed inverse-frequency class weights.\")\n    return {i: float(inv[i]) for i in range(num_classes)}\n\ncw_path = os.environ.get(CLASS_WEIGHTS_PATH_ENV, None)\nclass_weight = get_class_weight_dict(num_classes, try_path=cw_path)\nprint(\"Class weights mapping:\", class_weight)\n\n# --- load ResNet50 backbone (imagenet if available else local) ---\ndef load_resnet50_backbone(image_size):\n    try:\n        print(\"Loading ResNet50 imagenet weights...\")\n        base = tf.keras.applications.ResNet50(weights='imagenet', include_top=False,\n                                              input_shape=(image_size, image_size, 3), pooling='avg')\n        return base\n    except Exception as e:\n        print(\"Failed to load imagenet weights:\", e)\n    local_path = os.environ.get(LOCAL_WEIGHTS_ENV)\n    if local_path and os.path.isfile(os.path.expanduser(local_path)):\n        base = tf.keras.applications.ResNet50(weights=None, include_top=False,\n                                              input_shape=(image_size, image_size, 3), pooling='avg')\n        try:\n            base.load_weights(os.path.expanduser(local_path), by_name=True)\n            print(\"Loaded local weights.\")\n            return base\n        except Exception as e_local:\n            print(\"Failed local load:\", e_local)\n    raise RuntimeError(\"Could not load ResNet50 pretrained weights.\")\n\nbase_model = load_resnet50_backbone(IMAGE_SIZE)\n\n# --- build top/head ---\nbase_model.trainable = False\ninputs = tf.keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\nx = base_model(inputs, training=False)\nx = tf.keras.layers.Dense(32, activation='relu')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Dropout(0.4)(x)\n\noutputs = tf.keras.layers.Dense(num_classes, activation='softmax',\n                                kernel_regularizer=tf.keras.regularizers.l2(5e-4))(x)\nmodel = tf.keras.Model(inputs, outputs)\n\n# --------------------\n# Macro metrics (store confusion matrix as weight and compute macro average)\n# --------------------\nclass ConfMatMetricBase(tf.keras.metrics.Metric):\n    def __init__(self, num_classes, name, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.num_classes = int(num_classes)\n        self.cm = self.add_weight(name='confmat', shape=(self.num_classes, self.num_classes),\n                                  initializer='zeros', dtype=tf.float32)\n\n    def _update_confmat(self, y_true, y_pred):\n        preds = tf.argmax(y_pred, axis=-1, output_type=tf.int32)\n        y_true = tf.reshape(tf.cast(y_true, tf.int32), [-1])\n        preds = tf.reshape(tf.cast(preds, tf.int32), [-1])\n        batch_cm = tf.cast(tf.math.confusion_matrix(y_true, preds, num_classes=self.num_classes, dtype=tf.float32), tf.float32)\n        self.cm.assign_add(batch_cm)\n\nclass MacroPrecision(ConfMatMetricBase):\n    def __init__(self, num_classes, name='macro_precision', **kwargs):\n        super().__init__(num_classes, name, **kwargs)\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        self._update_confmat(y_true, y_pred)\n    def result(self):\n        tp = tf.linalg.tensor_diag_part(self.cm)\n        predicted = tf.reduce_sum(self.cm, axis=0)\n        precision_per_class = tf.where(predicted > 0, tp / predicted, tf.zeros_like(predicted))\n        return tf.reduce_mean(precision_per_class)\n    def reset_states(self):\n        self.cm.assign(tf.zeros_like(self.cm))\n\nclass MacroRecall(ConfMatMetricBase):\n    def __init__(self, num_classes, name='macro_recall', **kwargs):\n        super().__init__(num_classes, name, **kwargs)\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        self._update_confmat(y_true, y_pred)\n    def result(self):\n        tp = tf.linalg.tensor_diag_part(self.cm)\n        actual = tf.reduce_sum(self.cm, axis=1)\n        recall_per_class = tf.where(actual > 0, tp / actual, tf.zeros_like(actual))\n        return tf.reduce_mean(recall_per_class)\n    def reset_states(self):\n        self.cm.assign(tf.zeros_like(self.cm))\n\nclass MacroF1(ConfMatMetricBase):\n    def __init__(self, num_classes, name='macro_f1', **kwargs):\n        super().__init__(num_classes, name, **kwargs)\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        self._update_confmat(y_true, y_pred)\n    def result(self):\n        tp = tf.linalg.tensor_diag_part(self.cm)\n        predicted = tf.reduce_sum(self.cm, axis=0)\n        actual = tf.reduce_sum(self.cm, axis=1)\n        precision_per_class = tf.where(predicted > 0, tp / predicted, tf.zeros_like(predicted))\n        recall_per_class = tf.where(actual > 0, tp / actual, tf.zeros_like(actual))\n        f1_per_class = tf.where((precision_per_class + recall_per_class) > 0,\n                                2.0 * precision_per_class * recall_per_class / (precision_per_class + recall_per_class),\n                                tf.zeros_like(precision_per_class))\n        return tf.reduce_mean(f1_per_class)\n    def reset_states(self):\n        self.cm.assign(tf.zeros_like(self.cm))\n\n# compile for phase 1 (head only)\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'),\n             MacroPrecision(num_classes=num_classes),\n             MacroRecall(num_classes=num_classes),\n             MacroF1(num_classes=num_classes)]\n)\n\n# safe formatting helper and print callback\ndef safe_fmt(x, precision=4):\n    try:\n        if isinstance(x, (tf.Tensor, np.generic)):\n            x = x.numpy() if hasattr(x, \"numpy\") else float(x)\n        return f\"{float(x):.{precision}f}\"\n    except Exception:\n        return \"   N/A\"\n\nclass PrintMetricsCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        train_loss = logs.get('loss')\n        val_loss = logs.get('val_loss')\n        train_acc = logs.get('accuracy') or logs.get('sparse_categorical_accuracy')\n        val_acc = logs.get('val_accuracy') or logs.get('val_sparse_categorical_accuracy')\n        s = (f\"[Epoch {epoch+1:02d}] train_loss={safe_fmt(train_loss)} train_acc={safe_fmt(train_acc)} | \"\n             f\"val_loss={safe_fmt(val_loss)} val_acc={safe_fmt(val_acc)}\")\n        print(s)\n\nprint(\"Starting Phase 1 (train head)...\")\nhistory1 = model.fit(train_ds, validation_data=val_ds, epochs=FIRST_PHASE_EPOCHS, callbacks=[PrintMetricsCallback()],class_weight=class_weight)\n","metadata":{"papermill":{"duration":3595.684593,"end_time":"2025-09-10T17:01:48.083532","exception":false,"start_time":"2025-09-10T16:01:52.398939","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T00:41:42.566777Z","iopub.execute_input":"2025-09-12T00:41:42.567463Z","iopub.status.idle":"2025-09-12T01:40:38.459401Z","shell.execute_reply.started":"2025-09-12T00:41:42.567439Z","shell.execute_reply":"2025-09-12T01:40:38.458709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------ CELL 3 (modified) ------------------\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport tensorflow as tf\nimport os\n\n# Phase 2: unfreeze some of the backbone \nN_unfreeze = 50  \nfor layer in base_model.layers[:-N_unfreeze]:\n    layer.trainable = False\nfor layer in base_model.layers:\n    if isinstance(layer, tf.keras.layers.BatchNormalization):\n        layer.trainable = False\nbase_model.trainable = True\n\nINITIAL_LR = (1e-4)*(0.8)\nINITIAL_WEIGHT_DECAY = (1e-4)\n\ntry:\n    # TF 2.11+ may expose AdamW in either tf.keras.optimizers or tf.keras.optimizers.experimental\n    AdamW = getattr(tf.keras.optimizers, \"AdamW\", None) or getattr(tf.keras.optimizers, \"experimental\", None) and getattr(tf.keras.optimizers.experimental, \"AdamW\", None)\n    if AdamW is None:\n        raise AttributeError\n    optimizer = AdamW(learning_rate=INITIAL_LR, weight_decay=INITIAL_WEIGHT_DECAY)\n    print(\"Using AdamW optimizer with weight_decay.\")\nexcept Exception:\n    optimizer = tf.keras.optimizers.Adam(learning_rate=INITIAL_LR)\n    print(\"AdamW not found — using standard Adam (no optimizer weight decay). \"\n          \"Consider adding kernel_regularizer=l2(...) to layers if you want L2 regularization.\")\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\",   \n    factor=0.5,           # multiply LR by this factor when plateau\n    patience=2,           # number of epochs with no improvement before reducing LR\n    verbose=1,\n    min_lr=1e-7\n)\n\nearly_stop = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",   # stop when this stops improving\n    patience=6,          # how many epochs to wait after last improvement\n    restore_best_weights=True,\n    verbose=1\n)\n\nclass IncreaseWeightDecayOnLRDrop(tf.keras.callbacks.Callback):\n    def __init__(self, factor=1.5, max_weight_decay=1e-3):\n        super().__init__()\n        self.factor = factor\n        self.max_weight_decay = max_weight_decay\n        self._prev_lr = None\n\n    def on_train_begin(self, logs=None):\n        # capture initial lr\n        try:\n            self._prev_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n        except Exception:\n            # fallback for optimizers exposing 'lr'\n            try:\n                self._prev_lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n            except Exception:\n                self._prev_lr = None\n\n    def on_epoch_end(self, epoch, logs=None):\n        # get current lr value robustly\n        cur_lr = None\n        try:\n            cur_lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n        except Exception:\n            try:\n                cur_lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n            except Exception:\n                cur_lr = None\n\n        # if lr dropped since last epoch, try to bump weight decay\n        if self._prev_lr is not None and cur_lr is not None and cur_lr < self._prev_lr - 1e-12:\n            opt = self.model.optimizer\n            # try common attribute names for weight decay\n            wd_attr_names = [\"weight_decay\", \"decay\", \"weightDecay\"]\n            for name in wd_attr_names:\n                if hasattr(opt, name):\n                    try:\n                        current_wd = getattr(opt, name)\n                        # if it's a tf.Variable or a wrapper, attempt to extract a float\n                        try:\n                            current_wd_val = float(tf.keras.backend.get_value(current_wd))\n                        except Exception:\n                            current_wd_val = float(current_wd)\n                        new_wd = min(current_wd_val * self.factor, self.max_weight_decay)\n                        # try assign if tf.Variable-like\n                        try:\n                            tf.keras.backend.set_value(getattr(opt, name), new_wd)\n                        except Exception:\n                            try:\n                                setattr(opt, name, new_wd)\n                            except Exception:\n                                print(f\"[IncreaseWeightDecayOnLRDrop] Could not set {name} on optimizer.\")\n                        print(f\"[IncreaseWeightDecayOnLRDrop] LR decreased => increased {name} from {current_wd_val:.2e} to {new_wd:.2e}\")\n                        break\n                    except Exception as e:\n                        print(\"[IncreaseWeightDecayOnLRDrop] error while updating weight decay:\", e)\n            else:\n                # No known weight-decay attribute exposed\n                print(\"[IncreaseWeightDecayOnLRDrop] optimizer does not expose a writable weight-decay attribute; skipping.\")\n        # update prev_lr for next epoch\n        if cur_lr is not None:\n            self._prev_lr = cur_lr\n\n# Instantiate the optional callback (set to False to disable)\nuse_increase_wd_callback = True\nincrease_wd_cb = IncreaseWeightDecayOnLRDrop(factor=1.5, max_weight_decay=1e-3) if use_increase_wd_callback else None\n\n# -------------------------\n# Compile model\n# -------------------------\nmodel.compile(\n    optimizer=optimizer,\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'),\n             MacroPrecision(num_classes=num_classes),\n             MacroRecall(num_classes=num_classes),\n             MacroF1(num_classes=num_classes)]\n)\n\n# -------------------------\n# Fit: note we no longer use an epoch-based LR scheduler\n# -------------------------\ncallbacks_list = [reduce_lr, early_stop, PrintMetricsCallback()]\nif increase_wd_cb is not None:\n    callbacks_list.append(increase_wd_cb)\n\nprint(f\"Starting Phase 2: fine-tuning (initial lr={INITIAL_LR:.1e}) ...\")\nhistory2 = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS,\n    initial_epoch=FIRST_PHASE_EPOCHS,\n    class_weight=class_weight,\n    callbacks=callbacks_list\n)\n\nprint(\"Training complete. Running final evaluation (no augmentation)...\")\n\n# (evaluation + confusion-matrix code follows unchanged)\n# create clean evaluation dataset for train/val (no augmentation)\ntrain_eval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    TRAIN_DIR, labels='inferred', label_mode='int', batch_size=BATCH_SIZE,\n    image_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=False\n).map(lambda x,y: prepare_for_eval(x,y), num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n\ntrain_eval = model.evaluate(train_eval_ds, verbose=0)\nif isinstance(train_eval, (list, tuple)) and len(train_eval) >= 2:\n    train_loss, train_acc = train_eval[0], train_eval[1]\nelse:\n    train_loss, train_acc = train_eval, None\n\nif val_ds:\n    val_eval = model.evaluate(val_ds, verbose=0)\n    if isinstance(val_eval, (list, tuple)) and len(val_eval) >= 2:\n        val_loss, val_acc = val_eval[0], val_eval[1]\n    else:\n        val_loss, val_acc = val_eval, None\nelse:\n    val_loss = val_acc = None\n\nprint(f\"Train - loss: {train_loss:.4f} acc: {train_acc:.4f}\" if train_acc is not None else f\"Train - loss: {train_loss:.4f}\")\nif val_ds and val_acc is not None:\n    print(f\"Val   - loss: {val_loss:.4f} acc: {val_acc:.4f}\")\nelif val_ds:\n    print(f\"Val   - loss: {val_loss:.4f}\")\nelse:\n    print(\"Val   - N/A\")\n\n# -------------------- validation confusion matrix (seaborn 'Blues') --------------------\ndef plot_confusion_matrix_seaborn(model, dataset, class_names, title=\"Confusion Matrix (Val)\", save_path='confusion_matrix_val.png'):\n    ys, preds = [], []\n    for batch_x, batch_y in dataset:\n        batch_preds = model.predict(batch_x, verbose=0)\n        pred_labels = np.argmax(batch_preds, axis=-1)\n        ys.append(batch_y.numpy().reshape(-1))\n        preds.append(pred_labels.reshape(-1))\n    if not ys:\n        print(\"No validation data to plot.\")\n        return\n    y_true = np.concatenate(ys)\n    y_pred = np.concatenate(preds)\n\n    from sklearn.metrics import confusion_matrix\n    cm = confusion_matrix(y_true, y_pred)\n\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n    try:\n        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        print(\"Saved:\", os.path.abspath(save_path))\n    except Exception as e:\n        print(\"Could not save val confusion image:\", e)\n    plt.close()\n\nif val_ds:\n    plot_confusion_matrix_seaborn(model, val_ds, class_names, title=\"Confusion Matrix (Validation)\")\n","metadata":{"papermill":{"duration":3972.251645,"end_time":"2025-09-10T18:08:00.679957","exception":false,"start_time":"2025-09-10T17:01:48.428312","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T01:40:38.461181Z","iopub.execute_input":"2025-09-12T01:40:38.461427Z","iopub.status.idle":"2025-09-12T03:46:45.619619Z","shell.execute_reply.started":"2025-09-12T01:40:38.461409Z","shell.execute_reply":"2025-09-12T03:46:45.61894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 4 (updated) — test dataset evaluation + classification report + seaborn confusion matrix + test accuracy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# prefer OUT_ROOT/test if exists, otherwise use VAL_DIR as fallback\nTEST_DIR = os.path.join(OUT_ROOT, 'test')\nprint(\"Using test directory:\", TEST_DIR)\n\ntest_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    TEST_DIR, labels='inferred', label_mode='int', batch_size=BATCH_SIZE,\n    image_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=False\n).map(lambda x,y: prepare_for_eval(x,y), num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n\ny_true, y_pred = [], []\nfor batch_x, batch_y in test_ds:\n    preds = model.predict(batch_x, verbose=0)\n    pred_labels = np.argmax(preds, axis=-1)\n    y_true.append(batch_y.numpy().reshape(-1))\n    y_pred.append(pred_labels.reshape(-1))\n\nif len(y_true) == 0:\n    print(\"No samples in test dataset.\")\nelse:\n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n\n    # numeric metrics\n    test_acc = float(np.mean(y_true == y_pred))\n    print(f\"\\nTest accuracy (raw): {test_acc:.4f}\")\n\n    # try model.evaluate to print loss + compiled metrics (if you want the model's metrics)\n    try:\n        eval_res = model.evaluate(test_ds, verbose=0)\n        if isinstance(eval_res, (list, tuple)):\n            print(\"\\nModel.evaluate results:\")\n            for name, val in zip(getattr(model, 'metrics_names', []), eval_res):\n                try:\n                    print(f\"  {name}: {val:.4f}\")\n                except Exception:\n                    print(f\"  {name}: {val}\")\n        else:\n            print(f\"\\nModel.evaluate result: {eval_res}\")\n    except Exception as e:\n        print(\"model.evaluate failed (safe to ignore):\", e)\n\n    from sklearn.metrics import confusion_matrix, classification_report\n    cm = confusion_matrix(y_true, y_pred)\n\n    print(\"\\nClassification report:\")\n    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n\n    # seaborn-style plot exactly like your snippet\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(\"Confusion Matrix (Test)\")\n    plt.tight_layout()\n    plt.show()\n\n    try:\n        out_path = 'confusion_matrix_test.png'\n        plt.savefig(out_path, dpi=150, bbox_inches='tight')\n        print(\"Saved confusion matrix image to:\", os.path.abspath(out_path))\n    except Exception as e:\n        print(\"Could not save confusion matrix image:\", e)\n    plt.close()\n","metadata":{"papermill":{"duration":33.234704,"end_time":"2025-09-10T18:08:34.602052","exception":false,"start_time":"2025-09-10T18:08:01.367348","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T03:46:45.620512Z","iopub.execute_input":"2025-09-12T03:46:45.62092Z","iopub.status.idle":"2025-09-12T03:47:17.398232Z","shell.execute_reply.started":"2025-09-12T03:46:45.620903Z","shell.execute_reply":"2025-09-12T03:47:17.397407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.723516,"end_time":"2025-09-10T18:08:35.999056","exception":false,"start_time":"2025-09-10T18:08:35.27554","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}